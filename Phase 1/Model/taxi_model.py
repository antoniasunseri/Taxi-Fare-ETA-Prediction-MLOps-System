# -*- coding: utf-8 -*-
"""taxi_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLDc73NXZTz2XpdHJKv7F9S25no0i_oe
"""

import requests
import pandas as pd

# Define the SODA2 API endpoint
api_url = "https://data.cityofnewyork.us/resource/t29m-gskq.json"

try:
    # Make a GET request to the API
    response = requests.get(api_url)
    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

    # Convert the JSON response to a list of dictionaries
    data = response.json()

    # Create a pandas DataFrame from the data
    df = pd.DataFrame(data)

    print("NYC taxi data loaded successfully from SODA2 API. Displaying the first few rows:")
    print(df.head())
except requests.exceptions.RequestException as e:
    print(f"Error fetching data from API: {e}")
    print("No NYC taxi data could be loaded at this time.")
    df = pd.DataFrame() # Initialize an empty DataFrame to avoid further errors
except ValueError as e:
    print(f"Error parsing JSON response: {e}")
    print("No NYC taxi data could be loaded at this time.")
    df = pd.DataFrame() # Initialize an empty DataFrame to avoid further errors

print('DataFrame Info:')
df.info()

print('\nDescriptive statistics for numerical columns:')
print(df.describe())

df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')
df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')

print('Datetime columns converted successfully. Displaying info for datetime columns:')
print(df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']].info())

df['travel_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60
print('Travel duration calculated successfully. Displaying the first few rows with the new column:')
print(df[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'travel_duration']].head())

df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour
df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek

print('Extracted pickup hour and day of week successfully. Displaying the first few rows with new columns:')
print(df[['tpep_pickup_datetime', 'pickup_hour', 'pickup_day_of_week']].head())

numeric_cols = ['passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

print('Converted specified columns to numeric type. Displaying info for these columns:')
print(df[numeric_cols].info())

print('Missing values in each column:')
print(df.isnull().sum())

print('\nDescriptive statistics for key numerical features:')
print(df[['travel_duration', 'trip_distance', 'fare_amount', 'passenger_count']].describe())

df_filtered = df[(df['fare_amount'] > 0) &
                 (df['passenger_count'] > 0) &
                 (df['trip_distance'] > 0) &
                 (df['travel_duration'] > 0)]

print(f"Original DataFrame shape: {df.shape}")
print(f"Filtered DataFrame shape: {df_filtered.shape}")
print('\nDescriptive statistics after filtering erroneous entries:')
print(df_filtered[['travel_duration', 'trip_distance', 'fare_amount', 'passenger_count']].describe())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.boxplot(y=df_filtered['travel_duration'])
plt.title('Box Plot of Travel Duration')

plt.subplot(2, 2, 2)
sns.boxplot(y=df_filtered['trip_distance'])
plt.title('Box Plot of Trip Distance')

plt.subplot(2, 2, 3)
sns.boxplot(y=df_filtered['fare_amount'])
plt.title('Box Plot of Fare Amount')

plt.subplot(2, 2, 4)
sns.boxplot(y=df_filtered['passenger_count'])
plt.title('Box Plot of Passenger Count')

plt.tight_layout()
plt.show()

print("Box plots displayed for key numerical features to visualize outliers.")

for col in ['travel_duration', 'trip_distance', 'fare_amount']:
    upper_bound = df_filtered[col].quantile(0.99)
    df_filtered = df_filtered[df_filtered[col] <= upper_bound]
    print(f"Capped '{col}' at its 99th percentile: {upper_bound:.2f}")

print(f"Filtered DataFrame shape after capping outliers: {df_filtered.shape}")
print('\nDescriptive statistics after capping outliers:')
print(df_filtered[['travel_duration', 'trip_distance', 'fare_amount', 'passenger_count']].describe())

categorical_cols = ['vendorid', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type']
df_encoded = pd.get_dummies(df_filtered, columns=categorical_cols, drop_first=True)

print('Categorical features one-hot encoded successfully. Displaying the first few rows of the encoded DataFrame:')
print(df_encoded.head())

from sklearn.preprocessing import StandardScaler

# The target variable `travel_duration` should typically not be scaled before model training, or scaled separately if desired for certain models.
# For now, we will exclude it from feature scaling.

# Identify all current columns in df_encoded
all_cols = df_encoded.columns.tolist()

# Exclude datetime columns (already handled), target variable, and the one-hot encoded columns (already created and don't need scaling)
# The categorical_cols are effectively removed and replaced by one-hot encoded columns, so they are not in `df_encoded` in their original form.


numerical_features_to_scale = [
    'passenger_count',
    'trip_distance',
    'fare_amount',
    'extra',
    'mta_tax',
    'tip_amount',
    'tolls_amount',
    'improvement_surcharge',
    'total_amount',
    'pickup_hour',
    'pickup_day_of_week'
]

# Filter to include only those columns that actually exist in df_encoded and are not the target or date/time itself
# We already dropped 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' in the previous step.
# Also 'travel_duration' is the target, so it should not be scaled with features.

numerical_features_to_scale = [col for col in numerical_features_to_scale if col in df_encoded.columns]

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply scaling to the selected numerical features
df_encoded[numerical_features_to_scale] = scaler.fit_transform(df_encoded[numerical_features_to_scale])

print('Numerical features scaled successfully. Displaying the first few rows of the scaled features:')
print(df_encoded[numerical_features_to_scale].head())

location_cols = [col for col in df_encoded.columns if col.startswith('pulocationid_') or col.startswith('dolocationid_')]

# Define the features for X
selected_features = ['pickup_day_of_week', 'trip_distance', 'pickup_hour']

# Create the feature set X and target variable y
X = df_encoded[selected_features]
y = df_encoded['travel_duration']

print('Feature set X and target variable y created successfully.')
print(f'Shape of X: {X.shape}')
print(f'Shape of y: {y.shape}')
print('\nFirst 5 rows of X:')
print(X.head())
print('\nFirst 5 rows of y:')
print(y.head())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('Data split into training and testing sets successfully.')
print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of X_test: {X_test.shape}')
print(f'Shape of y_train: {y_train.shape}')
print(f'Shape of y_test: {y_test.shape}')

from sklearn.linear_model import LinearRegression

# Instantiate the LinearRegression model
model = LinearRegression()

# Train the model using the training data
model.fit(X_train, y_train)

print('Linear Regression model trained successfully.')

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate regression metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')

import wandb

# Install wandb if not already installed (uncomment the line below if needed)
# !pip install wandb

# Login to Weights & Biases (you will be prompted to enter your API key if not already logged in)
wandb.login()

# Initialize a new W&B run
# You can customize the project name and configuration parameters
run = wandb.init(project="nyc-taxi-fare-prediction",
                 config={
                     "model_type": "Linear Regression",
                     "test_size": 0.2,
                     "random_state": 42,
                     "features": X.columns.tolist() # Log the features used
                 })

# Log the evaluation metrics
wandb.log({
    "mean_absolute_error": mae,
    "mean_squared_error": mse,
    "root_mean_squared_error": rmse
})

print("Weights & Biases run initialized and metrics logged successfully.")

# Finish the W&B run
wandb.finish()

import joblib

# Define the filename for the saved model
model_filename = 'linear_regression_model.joblib'

# Save the trained model using joblib.dump()
joblib.dump(model, model_filename)

print(f'Linear Regression model saved successfully to {model_filename}')

import wandb

# Initialize a new W&B run for tracking the model artifact
# Use a different project name to distinguish from the metrics logging run
run_artifact = wandb.init(project="nyc-taxi-model-predictions", job_type="model-logging")

# Create a wandb.Artifact instance
artifact = wandb.Artifact('linear-regression-model', type='model')

# Add the locally saved model file to the artifact
# The model_filename variable was defined in the previous step where the model was saved.
artifact.add_file(model_filename)

# Log the created artifact to Weights & Biases
wandb.log_artifact(artifact)

print(f"Weights & Biases artifact '{artifact.name}' (type: {artifact.type}) logged successfully.")

# Finish the W&B run for artifact logging
run_artifact.finish()

import wandb
import joblib

# Install wandb if not already installed (uncomment the line below if needed)
# !pip install wandb

# Login to Weights & Biases (you will be prompted to enter your API key if not already logged in)
wandb.login()

# Initialize a new W&B run
# You can customize the project name and configuration parameters
run = wandb.init(project="nyc-taxi-fare-prediction",
                 config={
                     "model_type": "Linear Regression",
                     "test_size": 0.2,
                     "random_state": 42,
                     "features": X.columns.tolist() # Log the features used
                 })

# Log the evaluation metrics
wandb.log({
    "mean_absolute_error": mae,
    "mean_squared_error": mse,
    "root_mean_squared_error": rmse
})

# Define the filename for the saved model (from previous step)
model_filename = 'linear_regression_model.joblib'

# Create a wandb.Artifact instance
artifact = wandb.Artifact('linear-regression-model', type='model')

# Add the locally saved model file to the artifact
artifact.add_file(model_filename)

# Log the created artifact to Weights & Biases within the same run
wandb.log_artifact(artifact)

print("Weights & Biases run initialized, metrics logged, and model artifact logged successfully.")

# Finish the W&B run
wandb.finish()